{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk1pyQfcuJI3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "class GasAnomalyDetector:\n",
        "    \"\"\"\n",
        "    Predictive analytics pipeline for detecting gas usage anomalies\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.feature_columns = []\n",
        "        self.scaler = None\n",
        "\n",
        "    def load_data(self, meter_logs_path, units_registry_path, anomaly_log_path=None):\n",
        "        \"\"\"\n",
        "        Load all datasets and merge\n",
        "        \"\"\"\n",
        "        # Load meter logs\n",
        "        self.meter_logs = pd.read_csv(meter_logs_path)\n",
        "        self.meter_logs['datetime'] = pd.to_datetime(\n",
        "            self.meter_logs['date'] + ' ' + self.meter_logs['time']\n",
        "        )\n",
        "\n",
        "        # Load units registry\n",
        "        self.units = pd.read_csv(units_registry_path)\n",
        "\n",
        "        # Load anomaly labels (ground truth)\n",
        "        if anomaly_log_path:\n",
        "            self.anomalies = pd.read_csv(anomaly_log_path)\n",
        "            self.anomalies['start_time'] = pd.to_datetime(self.anomalies['start_time'])\n",
        "        else:\n",
        "            self.anomalies = None\n",
        "\n",
        "        print(f\"Loaded {len(self.meter_logs)} meter readings\")\n",
        "        print(f\"Loaded {len(self.units)} units\")\n",
        "        if self.anomalies is not None:\n",
        "            print(f\"Loaded {len(self.anomalies)} labeled anomalies\")\n",
        "\n",
        "    def create_labels(self):\n",
        "        \"\"\"\n",
        "        Create binary labels from anomaly log (ground truth)\n",
        "        \"\"\"\n",
        "        self.meter_logs['label'] = 0\n",
        "        self.meter_logs['anomaly_type'] = None\n",
        "\n",
        "        if self.anomalies is None:\n",
        "            print(\"No anomaly labels provided - unsupervised mode\")\n",
        "            return\n",
        "\n",
        "        # Mark anomalous periods\n",
        "        for _, anomaly in self.anomalies.iterrows():\n",
        "            mask = (\n",
        "                (self.meter_logs['date'].str.contains(anomaly['meter_id'])) &\n",
        "                (self.meter_logs['datetime'] >= anomaly['start_time']) &\n",
        "                (self.meter_logs['datetime'] < anomaly['start_time'] +\n",
        "                 pd.Timedelta(hours=anomaly['duration_hours']))\n",
        "            )\n",
        "            self.meter_logs.loc[mask, 'label'] = 1\n",
        "            self.meter_logs.loc[mask, 'anomaly_type'] = anomaly['type']\n",
        "\n",
        "        print(f\"Labeled data: {self.meter_logs['label'].sum()} anomalous hours, \"\n",
        "              f\"{(self.meter_logs['label']==0).sum()} normal hours\")\n",
        "        print(f\"Anomaly rate: {self.meter_logs['label'].mean()*100:.2f}%\")\n",
        "\n",
        "    def engineer_features(self):\n",
        "        \"\"\"\n",
        "        Create ML features from raw meter data\n",
        "        \"\"\"\n",
        "        print(\"\\nEngineering features...\")\n",
        "\n",
        "        # Merge with units registry\n",
        "        df = self.meter_logs.merge(\n",
        "            self.units[['meter_id', 'home_sqft', 'occupancy']],\n",
        "            left_on='date',  # Assuming 'date' contains meter_id info\n",
        "            right_on='meter_id',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Time-based features\n",
        "        df['hour'] = df['datetime'].dt.hour\n",
        "        df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
        "        df['month'] = df['datetime'].dt.month\n",
        "        df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)\n",
        "\n",
        "        # Usage pattern features\n",
        "        df = df.sort_values(['date', 'datetime'])\n",
        "\n",
        "        # Rolling statistics (by meter)\n",
        "        df['usage_ma_3h'] = df.groupby('date')['usage_therms'].transform(\n",
        "            lambda x: x.rolling(3, min_periods=1).mean()\n",
        "        )\n",
        "        df['usage_ma_6h'] = df.groupby('date')['usage_therms'].transform(\n",
        "            lambda x: x.rolling(6, min_periods=1).mean()\n",
        "        )\n",
        "        df['usage_ma_24h'] = df.groupby('date')['usage_therms'].transform(\n",
        "            lambda x: x.rolling(24, min_periods=1).mean()\n",
        "        )\n",
        "        df['usage_std_6h'] = df.groupby('date')['usage_therms'].transform(\n",
        "            lambda x: x.rolling(6, min_periods=1).std()\n",
        "        ).fillna(0)\n",
        "        df['usage_std_24h'] = df.groupby('date')['usage_therms'].transform(\n",
        "            lambda x: x.rolling(24, min_periods=1).std()\n",
        "        ).fillna(0)\n",
        "\n",
        "        # Delta features\n",
        "        df['usage_delta'] = df.groupby('date')['usage_therms'].diff().fillna(0)\n",
        "        df['usage_delta_abs'] = df['usage_delta'].abs()\n",
        "\n",
        "        # Consecutive non-zero hours (for continuous leak detection)\n",
        "        df['is_nonzero'] = (df['usage_therms'] > 0).astype(int)\n",
        "        df['nonzero_run'] = df.groupby('date')['is_nonzero'].transform(\n",
        "            lambda x: x.rolling(6, min_periods=1).sum()\n",
        "        )\n",
        "\n",
        "        # Temperature correlation (if temp available)\n",
        "        if 'temp' in df.columns:\n",
        "            df['temp_usage_interaction'] = df['temp'] * df['usage_therms']\n",
        "            df['heating_expected'] = (df['temp'] < 65).astype(int)\n",
        "            df['usage_vs_temp'] = np.where(\n",
        "                df['heating_expected'] == 1,\n",
        "                df['usage_therms'] / (70 - df['temp'].clip(upper=65)),\n",
        "                0\n",
        "            )\n",
        "\n",
        "        # Household context features\n",
        "        df['usage_per_sqft'] = df['usage_therms'] / df['home_sqft']\n",
        "        df['usage_per_person'] = df['usage_therms'] / df['occupancy']\n",
        "\n",
        "        # Time-of-day anomaly features\n",
        "        df['is_overnight'] = df['hour'].between(0, 5).astype(int)\n",
        "        df['is_cooking_time'] = df['hour'].isin([7, 11, 18, 19]).astype(int)\n",
        "        df['unusual_time_usage'] = np.where(\n",
        "            (df['is_overnight'] == 1) & (df['usage_therms'] > 0.05),\n",
        "            1, 0\n",
        "        )\n",
        "\n",
        "        # Statistical anomaly score (burst detection)\n",
        "        df['zscore_24h'] = np.where(\n",
        "            df['usage_std_24h'] > 0,\n",
        "            (df['usage_therms'] - df['usage_ma_24h']) / df['usage_std_24h'],\n",
        "            0\n",
        "        )\n",
        "        df['is_statistical_outlier'] = (df['zscore_24h'].abs() > 3).astype(int)\n",
        "\n",
        "        self.features_df = df\n",
        "\n",
        "        self.feature_columns = [\n",
        "            'hour', 'day_of_week', 'is_weekend', 'month', 'is_winter',\n",
        "            'usage_therms', 'usage_ma_3h', 'usage_ma_6h', 'usage_ma_24h',\n",
        "            'usage_std_6h', 'usage_std_24h', 'usage_delta', 'usage_delta_abs',\n",
        "            'nonzero_run', 'usage_per_sqft', 'usage_per_person',\n",
        "            'is_overnight', 'is_cooking_time', 'unusual_time_usage',\n",
        "            'zscore_24h', 'is_statistical_outlier',\n",
        "            'home_sqft', 'occupancy'\n",
        "        ]\n",
        "\n",
        "        if 'temp' in df.columns:\n",
        "            self.feature_columns.extend(['temp', 'heating_expected', 'usage_vs_temp'])\n",
        "\n",
        "        print(f\"Created {len(self.feature_columns)} features\")\n",
        "        return self.features_df\n",
        "\n",
        "    def train_model(self, test_size=0.2):\n",
        "        \"\"\"\n",
        "        Train LightGBM anomaly detection model\n",
        "        \"\"\"\n",
        "        print(\"\\nTraining anomaly detection model...\")\n",
        "\n",
        "        # Prepare data\n",
        "        X = self.features_df[self.feature_columns].fillna(0)\n",
        "        y = self.features_df['label']\n",
        "\n",
        "        # Train/test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Handle class imbalance\n",
        "        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "        # LightGBM parameters\n",
        "        params = {\n",
        "            'objective': 'binary',\n",
        "            'metric': 'auc',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'num_leaves': 31,\n",
        "            'learning_rate': 0.05,\n",
        "            'feature_fraction': 0.8,\n",
        "            'bagging_fraction': 0.8,\n",
        "            'bagging_freq': 5,\n",
        "            'scale_pos_weight': scale_pos_weight,\n",
        "            'verbose': -1\n",
        "        }\n",
        "\n",
        "        # Train\n",
        "        train_data = lgb.Dataset(X_train, label=y_train)\n",
        "        valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "\n",
        "        self.model = lgb.train(\n",
        "            params,\n",
        "            train_data,\n",
        "            num_boost_round=200,\n",
        "            valid_sets=[train_data, valid_data],\n",
        "            valid_names=['train', 'valid'],\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=20)]\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred_proba = self.model.predict(X_test, num_iteration=self.model.best_iteration)\n",
        "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"MODEL PERFORMANCE\")\n",
        "        print(\"=\"*60)\n",
        "        print(classification_report(y_test, y_pred, target_names=['Normal', 'Anomaly']))\n",
        "        print(f\"\\nROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(cm)\n",
        "\n",
        "        # Feature importance\n",
        "        self.plot_feature_importance()\n",
        "\n",
        "        return X_test, y_test, y_pred_proba\n",
        "\n",
        "    def plot_feature_importance(self, top_n=20):\n",
        "        \"\"\"\n",
        "        Visualize most important features\n",
        "        \"\"\"\n",
        "        importance = self.model.feature_importance(importance_type='gain')\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': self.feature_columns,\n",
        "            'importance': importance\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.barh(feature_importance['feature'].head(top_n),\n",
        "                 feature_importance['importance'].head(top_n))\n",
        "        plt.xlabel('Importance (Gain)')\n",
        "        plt.title(f'Top {top_n} Most Important Features')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"\\n✓ Feature importance plot saved as 'feature_importance.png'\")\n",
        "\n",
        "    def detect_anomalies_by_type(self, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Classify detected anomalies by type using rule-based logic\n",
        "        \"\"\"\n",
        "        print(\"\\nClassifying anomaly types...\")\n",
        "\n",
        "        # Get predictions\n",
        "        X = self.features_df[self.feature_columns].fillna(0)\n",
        "        anomaly_scores = self.model.predict(X, num_iteration=self.model.best_iteration)\n",
        "\n",
        "        self.features_df['anomaly_score'] = anomaly_scores\n",
        "        self.features_df['predicted_anomaly'] = (anomaly_scores > threshold).astype(int)\n",
        "\n",
        "        # Classify by type using rule-based logic\n",
        "        def classify_anomaly_type(row):\n",
        "            if row['predicted_anomaly'] == 0:\n",
        "                return 'normal'\n",
        "\n",
        "            # Continuous use leak\n",
        "            if row['nonzero_run'] >= 6 and row['usage_std_6h'] < 0.02:\n",
        "                return 'continuous_leak'\n",
        "\n",
        "            # Burst leak\n",
        "            if row['zscore_24h'] > 3 and row['usage_therms'] > 0.8:\n",
        "                return 'burst_leak'\n",
        "\n",
        "            # Time anomaly\n",
        "            if row['unusual_time_usage'] == 1:\n",
        "                return 'time_anomaly'\n",
        "\n",
        "            # Stove left on (evening + sustained)\n",
        "            if (row['hour'] >= 17) and (row['nonzero_run'] >= 3) and (row['usage_therms'] > 0.3):\n",
        "                return 'stove_left_on'\n",
        "\n",
        "            return 'general_anomaly'\n",
        "\n",
        "        self.features_df['predicted_type'] = self.features_df.apply(classify_anomaly_type, axis=1)\n",
        "\n",
        "        # Summary\n",
        "        type_counts = self.features_df[\n",
        "            self.features_df['predicted_anomaly'] == 1\n",
        "        ]['predicted_type'].value_counts()\n",
        "\n",
        "        print(\"\\nPredicted Anomaly Types:\")\n",
        "        print(type_counts)\n",
        "\n",
        "        return self.features_df\n",
        "\n",
        "    def score_to_snowflake(self, output_path='anomaly_scores.csv'):\n",
        "        \"\"\"\n",
        "        Export anomaly scores for Snowflake ingestion\n",
        "        \"\"\"\n",
        "        output = self.features_df[[\n",
        "            'date', 'datetime', 'usage_therms',\n",
        "            'anomaly_score', 'predicted_anomaly', 'predicted_type'\n",
        "        ]].copy()\n",
        "\n",
        "        output.columns = ['meter_id', 'timestamp', 'usage_therms',\n",
        "                         'score', 'is_anomaly', 'anomaly_type']\n",
        "\n",
        "        output.to_csv(output_path, index=False)\n",
        "        print(f\"\\n✓ Anomaly scores exported to '{output_path}'\")\n",
        "        print(\"Ready for Snowflake ingestion into ANOMALY_SCORES_GAS table\")\n",
        "\n",
        "    def visualize_predictions(self, meter_id=None, days=7):\n",
        "        \"\"\"\n",
        "        Visualize predictions for a specific meter\n",
        "        \"\"\"\n",
        "        if meter_id:\n",
        "            df = self.features_df[self.features_df['date'].str.contains(meter_id)]\n",
        "        else:\n",
        "            df = self.features_df\n",
        "\n",
        "        df = df.head(days * 24)  # First N days\n",
        "\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(15, 10), sharex=True)\n",
        "\n",
        "        # Usage pattern\n",
        "        axes[0].plot(df['datetime'], df['usage_therms'], label='Actual Usage', alpha=0.7)\n",
        "        axes[0].fill_between(df['datetime'], 0, df['usage_therms'],\n",
        "                             where=(df['predicted_anomaly']==1),\n",
        "                             color='red', alpha=0.3, label='Predicted Anomaly')\n",
        "        axes[0].set_ylabel('Usage (therms/hour)')\n",
        "        axes[0].set_title('Gas Usage Pattern with Anomaly Detection')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(alpha=0.3)\n",
        "\n",
        "        # Anomaly score\n",
        "        axes[1].plot(df['datetime'], df['anomaly_score'], color='orange', label='Anomaly Score')\n",
        "        axes[1].axhline(y=0.5, color='red', linestyle='--', label='Threshold')\n",
        "        axes[1].set_ylabel('Anomaly Score')\n",
        "        axes[1].set_title('Model Confidence Score')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(alpha=0.3)\n",
        "\n",
        "        # Temperature (if available)\n",
        "        if 'temp' in df.columns:\n",
        "            axes[2].plot(df['datetime'], df['temp'], color='blue', label='Temperature')\n",
        "            axes[2].set_ylabel('Temperature (°F)')\n",
        "            axes[2].set_xlabel('Time')\n",
        "            axes[2].set_title('Temperature Context')\n",
        "            axes[2].legend()\n",
        "            axes[2].grid(alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('anomaly_predictions.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"\\n✓ Prediction visualization saved as 'anomaly_predictions.png'\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize detector\n",
        "    detector = GasAnomalyDetector()\n",
        "\n",
        "    # Load data\n",
        "    detector.load_data(\n",
        "        meter_logs_path='data/synthetic/meter_logs_synthetic.csv',\n",
        "        units_registry_path='data/synthetic/units_gas_synthetic.csv',\n",
        "        anomaly_log_path='data/synthetic/anomaly_log.csv'\n",
        "    )\n",
        "\n",
        "    # Create labels from ground truth\n",
        "    detector.create_labels()\n",
        "\n",
        "    # Engineer features\n",
        "    detector.engineer_features()\n",
        "\n",
        "    # Train model\n",
        "    X_test, y_test, y_pred = detector.train_model(test_size=0.2)\n",
        "\n",
        "    # Detect and classify anomalies\n",
        "    detector.detect_anomalies_by_type(threshold=0.5)\n",
        "\n",
        "    # Export scores for Snowflake\n",
        "    detector.score_to_snowflake('anomaly_scores_for_snowflake.csv')\n",
        "\n",
        "    # Visualize results\n",
        "    detector.visualize_predictions(days=7)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PREDICTIVE ANALYTICS PIPELINE COMPLETE\")\n",
        "    print(\"=\"*60)"
      ]
    }
  ]
}